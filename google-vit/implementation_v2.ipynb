{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    ViTForImageClassification,\n",
    "    ViTConfig,\n",
    "    ViTImageProcessor\n",
    ")\n",
    "from datasets import (\n",
    "    load_dataset,\n",
    "    Dataset,\n",
    "    concatenate_datasets,\n",
    "    Features,\n",
    "    ClassLabel,\n",
    "    Image as DatasetsImage\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# 1. Load and Modify the Pre-trained ViT Model\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google/vit-base-patch16-224'  # Ensure using the correct model variant\n",
    "config = ViTConfig.from_pretrained(model_name)\n",
    "original_num_labels = config.num_labels\n",
    "print(f\"Original number of labels in the model config: {original_num_labels}\")\n",
    "\n",
    "# Assuming you're fine-tuning on ImageNet-1k with 1000 classes and adding 1 new class\n",
    "desired_num_labels = 1001  # 1000 original + 1 new\n",
    "config.num_labels = desired_num_labels\n",
    "\n",
    "# Load the pre-trained ViT model with the updated configuration\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True  # Allows loading models with different classifier sizes\n",
    ")\n",
    "\n",
    "# Verify and update the classifier layer\n",
    "if model.classifier.weight.size(0) == desired_num_labels:\n",
    "    print(f\"Classifier layer successfully updated to {desired_num_labels} classes.\")\n",
    "    # Initialize the new class's weights to zero to start with neutral predictions\n",
    "    with torch.no_grad():\n",
    "        nn.init.zeros_(model.classifier.weight[-1])\n",
    "else:\n",
    "    raise ValueError(\"Classifier layer was not updated correctly.\")\n",
    "\n",
    "# Set the main input name to \"pixel_values\" as expected by ViT models\n",
    "model.config.main_input_name = \"pixel_values\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# 2. Integrate LoRA with Standard Configuration\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target modules within the ViT encoder layers where LoRA will be applied\n",
    "target_modules = []\n",
    "for i in range(config.num_hidden_layers):\n",
    "    target_modules.extend([\n",
    "        f\"vit.encoder.layer.{i}.attention.attention.query\",\n",
    "        f\"vit.encoder.layer.{i}.attention.attention.key\",\n",
    "        f\"vit.encoder.layer.{i}.attention.attention.value\",\n",
    "        f\"vit.encoder.layer.{i}.attention.output.dense\",\n",
    "        f\"vit.encoder.layer.{i}.intermediate.dense\",\n",
    "        f\"vit.encoder.layer.{i}.output.dense\",\n",
    "    ])\n",
    "\n",
    "# Verify that all target modules exist within the model\n",
    "existing_modules = set()\n",
    "for name, module in model.named_modules():\n",
    "    existing_modules.add(name)\n",
    "\n",
    "for tm in target_modules:\n",
    "    if tm in existing_modules:\n",
    "        print(f\"Module '{tm}' found in the model.\")\n",
    "    else:\n",
    "        print(f\"Module '{tm}' NOT found in the model. Check the module naming.\")\n",
    "\n",
    "# Define LoRA configuration with standard initialization\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,  # Using FEATURE_EXTRACTION as IMAGE_CLASSIFICATION is unavailable\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=target_modules,\n",
    "    init_lora_weights=\"gaussian\"  # Standard initialization: \"gaussian\" or \"kaiming\"\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"LoRA integrated into the model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# 3. Override the Forward Method to Prevent 'input_ids'\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_forward(self, **kwargs):\n",
    "    \"\"\"\n",
    "    Custom forward method to ensure only 'pixel_values' and 'labels' are passed to the base model.\n",
    "    Removes 'input_ids' if present to prevent TypeError.\n",
    "    \"\"\"\n",
    "    # Remove 'input_ids' if present\n",
    "    kwargs.pop('input_ids', None)\n",
    "    # Ensure 'pixel_values' is present\n",
    "    if 'pixel_values' not in kwargs:\n",
    "        raise ValueError(\"Missing 'pixel_values' in inputs.\")\n",
    "    return self.base_model(**kwargs)\n",
    "\n",
    "# Bind the custom forward method to the model instance\n",
    "model.forward = types.MethodType(custom_forward, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# 4. Prepare the Dataset\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and set your Hugging Face token if authentication is required\n",
    "# from huggingface_hub import login\n",
    "# login(token=\"your_hf_token\")  # Replace with your actual token\n",
    "\n",
    "# Load the original ImageNet dataset (Using a subset for demonstration)\n",
    "original_dataset = load_dataset('imagenet-1k', split='train[:1%]')  # Adjust the split as needed\n",
    "class_names = original_dataset.features['label'].names\n",
    "print(f\"Original number of classes: {len(class_names)}\")  # Should print 1000\n",
    "\n",
    "# Add the new class name\n",
    "class_names.append('stop_sign')\n",
    "\n",
    "# Define new features with the updated ClassLabel\n",
    "new_features = Features({\n",
    "    'image': DatasetsImage(),\n",
    "    'label': ClassLabel(names=class_names)\n",
    "})\n",
    "\n",
    "# Load new category images\n",
    "new_category_images = []\n",
    "new_category_label = len(class_names) - 1  # Index of the new class (1000)\n",
    "\n",
    "# Replace with your actual path to the new category images\n",
    "new_category_path = 'mapilary_stopsign/regulatory_stop_g1'\n",
    "\n",
    "# Iterate through the directory and load images\n",
    "for img_name in os.listdir(new_category_path):\n",
    "    img_path = os.path.join(new_category_path, img_name)\n",
    "    if os.path.isfile(img_path):\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            new_category_images.append({'image': image, 'label': new_category_label})\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "\n",
    "# Create a Dataset from the new category images\n",
    "new_dataset = Dataset.from_dict({\n",
    "    'image': [item['image'] for item in new_category_images],\n",
    "    'label': [item['label'] for item in new_category_images]\n",
    "}).cast(new_features)\n",
    "\n",
    "# Cast the original_dataset to the new ClassLabel\n",
    "original_dataset = original_dataset.cast(new_features)\n",
    "\n",
    "# Verify labels in original_dataset are within [0, 999]\n",
    "max_original_label = max(original_dataset['label'])\n",
    "assert max_original_label < 1000, f\"Original dataset labels exceed 999. Max label: {max_original_label}\"\n",
    "\n",
    "# Verify labels in new_dataset are exactly 1000\n",
    "all_new_labels = set(new_dataset['label'])\n",
    "assert all_new_labels == {1000}, f\"New dataset labels are not set to 1000. Found labels: {all_new_labels}\"\n",
    "\n",
    "# Combine the datasets\n",
    "combined_dataset = concatenate_datasets([original_dataset, new_dataset])\n",
    "\n",
    "# Shuffle the combined dataset to ensure a good mix of classes\n",
    "combined_dataset = combined_dataset.shuffle(seed=42)\n",
    "\n",
    "print(f\"Combined dataset size: {len(combined_dataset)}\")\n",
    "print(f\"Classes after addition: {combined_dataset.features['label'].names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# 5. Data Preprocessing\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the feature extractor with the pre-trained model's configuration\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Ensure standard ImageNet normalization\n",
    "feature_extractor.image_mean = [0.485, 0.456, 0.406]\n",
    "feature_extractor.image_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "def preprocess(examples):\n",
    "    \"\"\"\n",
    "    Preprocesses a batch of examples by converting images to RGB and applying the feature extractor.\n",
    "    \"\"\"\n",
    "    # Ensure all images are in RGB format\n",
    "    images = [img.convert('RGB') for img in examples['image']]\n",
    "    \n",
    "    # Apply feature extractor to obtain pixel_values\n",
    "    encoding = feature_extractor(\n",
    "        images=images,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'pixel_values': encoding['pixel_values'],\n",
    "        'labels': examples['label']\n",
    "    }\n",
    "\n",
    "# Apply the preprocessing to the combined dataset\n",
    "combined_dataset = combined_dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=['image']\n",
    ")\n",
    "\n",
    "# Set the dataset format to PyTorch tensors for efficient loading\n",
    "combined_dataset.set_format(type='torch', columns=['pixel_values', 'labels'])\n",
    "\n",
    "print(\"Dataset sample:\")\n",
    "print(combined_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# 6. Create DataLoaders\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and evaluation sets\n",
    "train_test_split = combined_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n",
    "\n",
    "# Initialize DataLoaders for training and evaluation\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# 7. Define Training Parameters and Optimizer\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the device to run the training on (GPU if available)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move the model to the selected device\n",
    "model.to(device)\n",
    "\n",
    "# Define the optimizer using standard AdamW\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)  # Using PyTorch's AdamW to avoid FutureWarning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# 8. Define Evaluation Function\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the provided dataloader and returns the accuracy.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Get predictions by selecting the class with the highest logit\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            # Collect predictions and labels\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# 9. Training Loop with Evaluation\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3  # Define the number of training epochs\n",
    "\n",
    "# Verify model's num_labels\n",
    "print(\"Model config num_labels:\", model.config.num_labels)  # Should print 1001\n",
    "\n",
    "# Verify classifier layer shape\n",
    "print(\"Classifier layer shape:\", model.classifier.weight.shape)  # Should print torch.Size([1001, 768])\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    loop = tqdm(train_dataloader, leave=True, desc=f'Epoch {epoch + 1}/{epochs}')\n",
    "    epoch_loss = 0\n",
    "    for batch in loop:\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Update progress bar\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1} - Average Training Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "    # Evaluation after each epoch\n",
    "    eval_accuracy = evaluate(model, eval_dataloader, device)\n",
    "    print(f\"Epoch {epoch + 1} - Evaluation Accuracy: {eval_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# 10. Save the Fine-Tuned Model\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = 'vit-lora-finetuned'\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Save the PEFT model (including LoRA adapters)\n",
    "model.save_pretrained(save_directory)\n",
    "model.config.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and LoRA adapters saved to '{save_directory}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# 11. Optional: Merge LoRA Weights into the Base Model for Efficient Inference\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you wish to merge the LoRA adapters into the base model to eliminate the need for separate adapter loading during inference:\n",
    "model.merge_and_unload()\n",
    "print(\"LoRA adapters merged into the base model.\")\n",
    "\n",
    "# Save the merged model separately if desired\n",
    "merged_save_directory = 'lora-google-vit-stop-sign'\n",
    "os.makedirs(merged_save_directory, exist_ok=True)\n",
    "model.save_pretrained(merged_save_directory)\n",
    "model.config.save_pretrained(merged_save_directory)\n",
    "print(f\"Merged model saved to '{merged_save_directory}'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
